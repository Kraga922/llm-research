{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95355629-36c4-4a11-a88d-ac99c78747f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6418eb6-3a9b-47f1-8190-109cae54364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from repe import repe_pipeline_registry, WrappedReadingVecModel\n",
    "repe_pipeline_registry()\n",
    "\n",
    "from utils import literary_openings_dataset, quotes_dataset, quote_completion_test, historical_year_test, extract_year, eval_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e43231-f623-4036-b9ad-7451947df18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5afcbcff53c46c28c6090862e435fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\", token=True).eval()\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False, token=True)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9cba9-e681-4398-8677-5ab4eb27841f",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b7a04ed-d118-472d-85c1-d93930349bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "rep_token = -1\n",
    "hidden_layers = list(range(-1, -model.config.num_hidden_layers, -1))\n",
    "n_difference = 1\n",
    "direction_method = 'pca'\n",
    "rep_reading_pipeline =  pipeline(\"rep-reading\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15224f4a-784a-4f56-9618-db9a0db8e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/memorization\"\n",
    "lit_train_data, lit_train_labels, _ = literary_openings_dataset(data_dir)\n",
    "quote_train_data, quote_train_labels, _ = quotes_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c728c33-9357-4b3a-9fc0-93ffa8e2aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import repe.rep_readers\n",
    "\n",
    "def safe_project_onto_direction(H, direction):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Convert to float32 tensors and move to the same device\n",
    "    H = torch.tensor(H, dtype=torch.float32, device=device) if not isinstance(H, torch.Tensor) else H.to(device=device, dtype=torch.float32)\n",
    "    direction = torch.tensor(direction, dtype=torch.float32, device=device) if not isinstance(direction, torch.Tensor) else direction.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    projection = torch.matmul(H, direction) / torch.norm(direction)\n",
    "    return projection\n",
    "\n",
    "# Monkey-patch the unsafe function\n",
    "repe.rep_readers.project_onto_direction = safe_project_onto_direction\n",
    "\n",
    "lit_rep_reader = rep_reading_pipeline.get_directions(\n",
    "    lit_train_data, \n",
    "    rep_token=rep_token, \n",
    "    hidden_layers=hidden_layers, \n",
    "    n_difference=n_difference, \n",
    "    train_labels=lit_train_labels, \n",
    "    direction_method=direction_method,\n",
    ")\n",
    "\n",
    "quote_rep_reader = rep_reading_pipeline.get_directions(\n",
    "    quote_train_data, \n",
    "    rep_token=rep_token, \n",
    "    hidden_layers=hidden_layers, \n",
    "    n_difference=n_difference, \n",
    "    train_labels=quote_train_labels, \n",
    "    direction_method=direction_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1372c-ab42-4ca3-bbc9-a54dfbb36389",
   "metadata": {},
   "source": [
    "## Quote Completions Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67304c55-195f-4853-b16c-fe4c7673b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early layers work\n",
    "layer_id = list(range(-1,-9,-1))\n",
    "\n",
    "block_name=\"decoder_block\"\n",
    "control_method=\"reading_vec\"\n",
    "batch_size=64\n",
    "coeff=2.0 # tune this parameter\n",
    "max_new_tokens=16\n",
    "\n",
    "### We do manually instead of rep_control_pipeline here as an example\n",
    "wrapped_model = WrappedReadingVecModel(model, tokenizer)\n",
    "wrapped_model.unwrap()\n",
    "# wrap model at desired layers and blocks\n",
    "wrapped_model.wrap_block(layer_id, block_name=block_name)\n",
    "inputs, targets = quote_completion_test(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87fc6dc-766a-490a-bc08-5d19954e5a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(wrapped_model.model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b14935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def apply_activations(wrapped_model, inputs, activations, batch_size, use_tqdm, tokenizer, **generation_kwargs):\n",
    "    generated = []\n",
    "    iterator = range(0, len(inputs), batch_size)\n",
    "    \n",
    "    if use_tqdm:\n",
    "        from tqdm import tqdm\n",
    "        iterator = tqdm(iterator)\n",
    "    \n",
    "    for i in iterator:\n",
    "        inputs_b = inputs[i:i+batch_size]\n",
    "        \n",
    "        tokenized_inputs = tokenizer(inputs_b, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = tokenized_inputs[\"input_ids\"].to(wrapped_model.model.device)\n",
    "        attention_mask = tokenized_inputs.get(\"attention_mask\", None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(wrapped_model.model.device)\n",
    "        \n",
    "        outputs = wrapped_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        decoded_outputs = [o.replace(inp, \"\", 1) for o, inp in zip(decoded_outputs, inputs_b)]\n",
    "        generated.extend(decoded_outputs)\n",
    "    \n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17e085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device detected. Using CPU fallback for compatibility.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepReader: literature openings\n",
      "No Control\n",
      "{'em': 0.7864077669902912, 'sim': 0.9123673641565934}\n",
      "+ Memorization\n",
      "{'em': 0.7961165048543689, 'sim': 0.9085250688554014}\n",
      "- Memorization\n",
      "{'em': 0.7766990291262136, 'sim': 0.9094798905612196}\n",
      "RepReader: quotes\n",
      "No Control\n",
      "{'em': 0.7864077669902912, 'sim': 0.9073351899397026}\n",
      "+ Memorization\n",
      "{'em': 0.7669902912621359, 'sim': 0.9034268602873515}\n",
      "- Memorization\n",
      "{'em': 0.7572815533980582, 'sim': 0.8820471857014356}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    padding_side=\"left\" \n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# This is to offload processing to CPU's for Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS device detected. Using CPU fallback for compatibility.\")\n",
    "    # Set environment variable to avoid MPS issues\n",
    "    import os\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    # Alternative: move model to CPU\n",
    "    device = 'cpu'\n",
    "else:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if hasattr(wrapped_model, 'model'):\n",
    "    wrapped_model.model = wrapped_model.model.to(device)\n",
    "\n",
    "for t, rep_reader in zip(['literature openings', 'quotes'], [lit_rep_reader, quote_rep_reader]):\n",
    "    activations = {}\n",
    "    for layer in layer_id:\n",
    "        # Use consistent dtype and device\n",
    "        direction_tensor = torch.tensor(\n",
    "            0 * coeff * rep_reader.directions[layer] * rep_reader.direction_signs[layer], \n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        activations[layer] = direction_tensor.half() if device != 'cpu' else direction_tensor\n",
    "        \n",
    "    print(\"RepReader:\", t)\n",
    "    print(\"No Control\")\n",
    "    baseline_outputs = apply_activations(\n",
    "        wrapped_model,\n",
    "        inputs, \n",
    "        activations,\n",
    "        batch_size=64,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        use_tqdm=False,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    print(eval_completions(baseline_outputs, targets))\n",
    "    \n",
    "    # + Memorization\n",
    "    activations = {}\n",
    "    for layer in layer_id:\n",
    "        direction_tensor = torch.tensor(\n",
    "            coeff * rep_reader.directions[layer] * rep_reader.direction_signs[layer], \n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        activations[layer] = direction_tensor.half() if device != 'cpu' else direction_tensor\n",
    "        \n",
    "    print(\"+ Memorization\")\n",
    "    pos_outputs = apply_activations(\n",
    "        wrapped_model,\n",
    "        inputs, \n",
    "        activations,\n",
    "        batch_size=64,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        use_tqdm=False,\n",
    "        tokenizer=tokenizer  \n",
    "    )\n",
    "    \n",
    "    print(eval_completions(pos_outputs, targets))\n",
    "    \n",
    "    activations = {}\n",
    "    for layer in layer_id:\n",
    "        direction_tensor = torch.tensor(\n",
    "            -coeff * rep_reader.directions[layer] * rep_reader.direction_signs[layer], \n",
    "            dtype=torch.float32\n",
    "        ).to(device)\n",
    "        activations[layer] = direction_tensor.half() if device != 'cpu' else direction_tensor\n",
    "        \n",
    "    print(\"- Memorization\")\n",
    "    neg_outputs = apply_activations(\n",
    "        wrapped_model,\n",
    "        inputs, \n",
    "        activations,\n",
    "        batch_size=64,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        use_tqdm=False,\n",
    "        tokenizer=tokenizer  # Add missing tokenizer parameter\n",
    "    )\n",
    "    print(eval_completions(neg_outputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72297467-78d6-4d6e-8cb2-9fdbafe77464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
