import argparse
import pickle
import torch
import numpy as np
from pathlib import Path
import sys
import os

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from neural_controllers import NeuralController
from utils import load_model
from toxic_chat import get_data, get_prompt
from direction_utils import compute_prediction_metrics, get_hidden_states
from sklearn.metrics import roc_auc_score

NEURAL_CONTROLLERS_DIR = os.environ.get('NEURAL_CONTROLLERS_DIR', '.')

def apply_steering_to_hidden_states(hidden_states, steering_vectors, coef, target_layers):
    """
    Apply steering vectors to hidden states
    
    Args:
        hidden_states: Dict of {layer_idx: tensor}
        steering_vectors: Dict of {layer_idx: steering_direction}
        coef: Steering coefficient
        target_layers: List of layers to apply steering to
    """
    steered_states = {}
    
    for layer_idx, states in hidden_states.items():
        if layer_idx in target_layers and layer_idx in steering_vectors:
            direction = steering_vectors[layer_idx]
            if isinstance(direction, torch.Tensor):
                direction = direction.to(states.device)
                # Ensure proper dimensions
                if len(direction.shape) == 1:
                    direction = direction.unsqueeze(0)
                if direction.shape[0] == 1 and states.shape[0] > 1:
                    direction = direction.expand(states.shape[0], -1)
                    
                steered_states[layer_idx] = states + coef * direction
            else:
                steered_states[layer_idx] = states
        else:
            steered_states[layer_idx] = states.clone()
            
    return steered_states

def evaluate_steering_effectiveness(model_name, steering_vectors_path, target_layers, coef_values, 
                                  prompt_version='empty', max_samples=None):
    """
    Test steering vector effectiveness
    
    Args:
        model_name: Name of the model
        steering_vectors_path: Path to pkl file with steering vectors
        target_layers: List of layer numbers to apply steering
        coef_values: List of coefficient values to test
        prompt_version: Prompt version ('empty' or 'v1')
        max_samples: Limit number of test samples
    """
    print(f"Loading model: {model_name}")
    language_model, tokenizer = load_model(model_name)
    
    print(f"Loading steering vectors from: {steering_vectors_path}")
    with open(steering_vectors_path, 'rb') as f:
        steering_data = pickle.load(f)
    
    # Extract steering vectors (adapt this based on your pkl structure)
    if isinstance(steering_data, dict):
        # If steering_data has multiple keys, use the first one or specify which key
        first_key = list(steering_data.keys())[0]
        if hasattr(steering_data[first_key], 'directions'):
            steering_vectors = steering_data[first_key].directions
        elif isinstance(steering_data[first_key], dict):
            steering_vectors = steering_data[first_key]
        else:
            steering_vectors = steering_data
    else:
        steering_vectors = steering_data
    
    print(f"Available steering vector layers: {list(steering_vectors.keys())}")
    print(f"Target layers: {target_layers}")
    print(f"Coefficient values: {coef_values}")
    
    # Get test data
    _, _, test_inputs, test_labels = get_data()
    
    if max_samples:
        test_inputs = test_inputs[:max_samples]
        test_labels = test_labels[:max_samples]
    
    print(f"Testing on {len(test_inputs)} samples")
    
    # Format inputs
    prompt = get_prompt(prompt_version)
    formatted_inputs = [prompt.format(query=x) for x in test_inputs]
    
    # Initialize controller for utilities
    controller = NeuralController(language_model, tokenizer, batch_size=1, n_components=1)
    formatted_inputs = [controller.format_prompt(x) for x in formatted_inputs]
    
    # Get hidden states
    print("Computing hidden states...")
    hidden_states = get_hidden_states(
        formatted_inputs, 
        language_model, 
        tokenizer,
        controller.hidden_layers,
        controller.hyperparams['forward_batch_size']
    )
    
    results = []
    test_labels_tensor = torch.tensor(test_labels).float()
    
    # Test baseline (no steering)
    print("\nTesting baseline (no steering)...")
    baseline_predictions = get_predictions_from_hidden_states(hidden_states, controller)
    baseline_metrics = compute_prediction_metrics(baseline_predictions, test_labels_tensor)
    
    print(f"Baseline AUC: {baseline_metrics['auc']:.4f}")
    print(f"Baseline F1: {baseline_metrics['f1']:.4f}")
    print(f"Baseline Accuracy: {baseline_metrics['acc']:.4f}")
    
    results.append({
        'coef': 0.0,
        'layers': target_layers,
        'metrics': baseline_metrics,
        'type': 'baseline'
    })
    
    # Test each coefficient
    for coef in coef_values:
        print(f"\nTesting coefficient: {coef}")
        
        # Apply steering
        steered_states = apply_steering_to_hidden_states(
            hidden_states, steering_vectors, coef, target_layers
        )
        
        # Get predictions
        predictions = get_predictions_from_hidden_states(steered_states, controller)
        
        # Compute metrics
        metrics = compute_prediction_metrics(predictions, test_labels_tensor)
        
        print(f"Coef {coef}: AUC={metrics['auc']:.4f}, F1={metrics['f1']:.4f}, Acc={metrics['acc']:.4f}")
        
        results.append({
            'coef': coef,
            'layers': target_layers,
            'metrics': metrics,
            'type': 'steered'
        })
    
    return results

def get_predictions_from_hidden_states(hidden_states, controller):
    """
    Simple method to get predictions from hidden states
    You may need to adapt this based on how your steering vectors work
    """
    # Use a simple linear probe on the last layer or mean of target layers
    # This is a placeholder - you might have a specific way to get predictions
    
    if isinstance(hidden_states, dict):
        # Get the last layer or use specific layers
        last_layer = max(hidden_states.keys())
        states = hidden_states[last_layer]
        
        # If 3D (batch, seq, hidden), take mean over sequence
        if len(states.shape) == 3:
            states = states.mean(dim=1)
        
        # Simple prediction: use mean activation or a learned linear layer
        # For now, just return the mean of the hidden states as a proxy
        predictions = states.mean(dim=-1, keepdim=True)
        
        # Normalize to 0-1 range
        predictions = torch.sigmoid(predictions)
        
    else:
        # If it's a single tensor
        if len(hidden_states.shape) == 3:
            hidden_states = hidden_states.mean(dim=1)
        predictions = torch.sigmoid(hidden_states.mean(dim=-1, keepdim=True))
    
    return predictions

def print_summary(results):
    """Print a summary of results"""
    print("\n" + "="*80)
    print("STEERING EFFECTIVENESS SUMMARY")
    print("="*80)
    
    baseline = [r for r in results if r['type'] == 'baseline'][0]
    steered = [r for r in results if r['type'] == 'steered']
    
    print(f"Baseline Performance:")
    print(f"  AUC: {baseline['metrics']['auc']:.4f}")
    print(f"  F1:  {baseline['metrics']['f1']:.4f}")
    print(f"  Acc: {baseline['metrics']['acc']:.4f}")
    
    print(f"\nSteering Results:")
    print(f"{'Coef':<8} {'AUC':<8} {'F1':<8} {'Acc':<8} {'AUC Δ':<8} {'F1 Δ':<8}")
    print("-" * 50)
    
    for result in steered:
        auc_delta = result['metrics']['auc'] - baseline['metrics']['auc']
        f1_delta = result['metrics']['f1'] - baseline['metrics']['f1']
        
        print(f"{result['coef']:<8.2f} "
              f"{result['metrics']['auc']:<8.4f} "
              f"{result['metrics']['f1']:<8.4f} "
              f"{result['metrics']['acc']:<8.4f} "
              f"{auc_delta:+<8.4f} "
              f"{f1_delta:+<8.4f}")
    
    # Find best configuration
    best_auc = max(steered, key=lambda x: x['metrics']['auc'])
    best_f1 = max(steered, key=lambda x: x['metrics']['f1'])
    
    print(f"\nBest AUC: {best_auc['metrics']['auc']:.4f} (coef={best_auc['coef']})")
    print(f"Best F1:  {best_f1['metrics']['f1']:.4f} (coef={best_f1['coef']})")

def main():
    parser = argparse.ArgumentParser(description='Test steering vector effectiveness')
    parser.add_argument('--model_name', type=str, required=True, 
                       help='Model name (e.g., llama_3.3_70b_4bit_it)')
    parser.add_argument('--steering_vectors_path', type=str, required=True,
                       help='Path to pickle file with steering vectors')
    parser.add_argument('--layers', nargs='+', type=int, required=True,
                       help='Layer numbers to apply steering (e.g., 15 20 25)')
    parser.add_argument('--coef_values', nargs='+', type=float, required=True,
                       help='Coefficient values to test (e.g., -1.0 -0.5 0.5 1.0)')
    parser.add_argument('--prompt_version', type=str, default='empty', choices=['empty', 'v1'],
                       help='Prompt version to use')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='Limit number of test samples (for faster testing)')
    parser.add_argument('--save_results', type=str, default=None,
                       help='Path to save detailed results')
    
    args = parser.parse_args()
    
    print("Steering Vector Effectiveness Test")
    print("="*50)
    print(f"Model: {args.model_name}")
    print(f"Steering vectors: {args.steering_vectors_path}")
    print(f"Target layers: {args.layers}")
    print(f"Coefficients: {args.coef_values}")
    print(f"Prompt version: {args.prompt_version}")
    if args.max_samples:
        print(f"Max samples: {args.max_samples}")
    
    # Run evaluation
    results = evaluate_steering_effectiveness(
        args.model_name,
        args.steering_vectors_path,
        args.layers,
        args.coef_values,
        args.prompt_version,
        args.max_samples
    )
    
    # Print summary
    print_summary(results)
    
    # Save results if requested
    if args.save_results:
        with open(args.save_results, 'wb') as f:
            pickle.dump(results, f)
        print(f"\nResults saved to: {args.save_results}")

if __name__ == '__main__':
    main()


# import argparse
# import sys
# import os
# from pathlib import Path
# import numpy as np
# import pickle
# import torch
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score, precision_recall_fscore_support
# from sklearn.calibration import calibration_curve
# import pandas as pd
# from scipy import stats

# # Add parent directory to path
# sys.path.append(str(Path(__file__).parent.parent))

# from neural_controllers import NeuralController
# from utils import load_model
# from datasets import load_dataset

# NEURAL_CONTROLLERS_DIR = os.environ.get('NEURAL_CONTROLLERS_DIR', './neural_controllers')

# def compute_prediction_metrics(predictions, labels):
#     """Compute comprehensive prediction metrics"""
#     # Convert to numpy if needed
#     if torch.is_tensor(predictions):
#         predictions = predictions.cpu().numpy()
#     if torch.is_tensor(labels):
#         labels = labels.cpu().numpy()
    
#     # Binary predictions
#     binary_preds = (predictions > 0.5).astype(int)
    
#     # Basic metrics
#     accuracy = accuracy_score(labels, binary_preds)
#     precision, recall, f1, _ = precision_recall_fscore_support(labels, binary_preds, average='binary')
    
#     # AUC
#     fpr, tpr, _ = roc_curve(labels, predictions)
#     roc_auc = auc(fpr, tpr)
    
#     # MSE
#     mse = np.mean((predictions - labels) ** 2)
    
#     return {
#         'auc': roc_auc,
#         'f1': f1,
#         'precision': precision,
#         'recall': recall,
#         'acc': accuracy,
#         'mse': mse
#     }

# def get_prompt(prompt_version):
#     if prompt_version == 'v1':
#         return 'Toxic content mainly includes offensiveness, sexism, and hateful speech. Does the following query contain toxic content? State yes or no. \n{query}'
#     elif prompt_version == 'empty':
#         return '{query}'
#     else:
#         raise ValueError(f"Invalid prompt version: {prompt_version}")

# def get_test_data(controller, prompt_version):
#     """Get test data for evaluation"""
#     # Load the dataset
#     ds = load_dataset("lmsys/toxic-chat", "toxicchat1123")
#     test_inputs = [x['user_input'] for x in ds['test']]
#     test_labels = [x['toxicity'] for x in ds['test']]
    
#     prompt = get_prompt(prompt_version)
    
#     # Wrap inputs with prompt
#     test_inputs = [prompt.format(query=x) for x in test_inputs]
    
#     # Format prompts with chat template
#     test_inputs = [controller.format_prompt(x) for x in test_inputs]
    
#     return test_inputs, np.array(test_labels)

# class FlexibleSteeringEvaluator:
#     def __init__(self):
#         self.evaluation_results = {}
        
#     def load_steering_vector(self, vector_path):
#         """Load steering vector from pickle file"""
#         with open(vector_path, 'rb') as f:
#             steering_data = pickle.load(f)
#         return steering_data
        
#     def evaluate_steering_vector(self, vector_path, model_name, control_method='rfm', 
#                                 prompt_version='empty', n_components=3):
#         """Evaluate a steering vector file directly"""
        
#         print(f"Loading model: {model_name}")
#         language_model, tokenizer = load_model(model=model_name)
        
#         print("Initializing controller...")
#         controller = NeuralController(
#             language_model,
#             tokenizer,
#             control_method=control_method,
#             batch_size=1,
#             n_components=n_components,
#         )
#         controller.name = model_name
        
#         # Load steering vector
#         print(f"Loading steering vector from: {vector_path}")
#         try:
#             # Try to load the controller directly
#             concept_name = Path(vector_path).stem.replace(f'_{model_name}', '')
#             controller.load(concept=concept_name, model_name=model_name, 
#                           path=str(Path(vector_path).parent))
#         except Exception as e:
#             print(f"Error loading as controller: {e}")
#             print("Trying to load as raw pickle file...")
#             steering_data = self.load_steering_vector(vector_path)
#             print(f"Steering data keys: {steering_data.keys() if isinstance(steering_data, dict) else type(steering_data)}")
#             # You might need to manually set the controller's directions here
#             # depending on how your steering vector is stored
        
#         # Get test data
#         print("Loading test data...")
#         test_inputs, test_labels = get_test_data(controller, prompt_version)
        
#         print(f"Test data: {len(test_inputs)} samples")
#         print(f"Example input: {test_inputs[0][:200]}...")
        
#         # Get hidden states for test data
#         print("Computing hidden states...")
#         from direction_utils import get_hidden_states
        
#         test_hidden_states = get_hidden_states(
#             test_inputs, language_model, tokenizer, 
#             controller.hidden_layers, 
#             controller.hyperparams['forward_batch_size']
#         )
        
#         # Make predictions using the controller
#         print("Making predictions...")
#         try:
#             # Use a dummy validation set (just use part of test set)
#             val_size = min(100, len(test_labels) // 4)
#             val_hidden_states = {layer: states[:val_size] for layer, states in test_hidden_states.items()}
#             val_labels = test_labels[:val_size]
            
#             # Dummy train data (required for evaluate_directions method)
#             train_hidden_states = val_hidden_states  # Using val as dummy train
#             train_labels = val_labels
            
#             _, _, _, test_predictions = controller.evaluate_directions(
#                 train_hidden_states, torch.tensor(train_labels).cuda().float(),
#                 val_hidden_states, torch.tensor(val_labels).cuda().float(),
#                 test_hidden_states, torch.tensor(test_labels).cuda().float(),
#                 n_components=n_components,
#                 agg_model=control_method,
#             )
            
#             # Extract predictions
#             if isinstance(test_predictions, dict):
#                 agg_predictions = test_predictions.get('aggregation', test_predictions.get('agg', None))
#                 best_layer_predictions = test_predictions.get('best_layer', test_predictions.get('best', None))
                
#                 if agg_predictions is not None:
#                     agg_predictions = agg_predictions.cpu().numpy()
#                 if best_layer_predictions is not None:
#                     best_layer_predictions = best_layer_predictions.cpu().numpy()
#             else:
#                 agg_predictions = test_predictions.cpu().numpy()
#                 best_layer_predictions = agg_predictions
                
#         except Exception as e:
#             print(f"Error during prediction: {e}")
#             print("Trying alternative prediction method...")
            
#             # Alternative: directly apply steering vectors if possible
#             # This would depend on your specific implementation
#             agg_predictions = np.random.random(len(test_labels))  # Placeholder
#             best_layer_predictions = np.random.random(len(test_labels))  # Placeholder
#             print("Using placeholder predictions - you may need to implement direct steering vector application")
        
#         return agg_predictions, best_layer_predictions, test_labels
        
#     def evaluate_predictions(self, predictions, true_labels, method_name):
#         """Comprehensive evaluation of predictions"""
        
#         # Convert to numpy arrays if needed
#         if torch.is_tensor(predictions):
#             predictions = predictions.cpu().numpy()
#         if torch.is_tensor(true_labels):
#             true_labels = true_labels.cpu().numpy()
            
#         results = {}
        
#         # 1. Basic Performance Metrics
#         metrics = compute_prediction_metrics(predictions, true_labels)
#         results['basic_metrics'] = metrics
        
#         # 2. Distribution Analysis
#         toxic_preds = predictions[true_labels == 1]
#         non_toxic_preds = predictions[true_labels == 0]
        
#         if len(toxic_preds) > 0 and len(non_toxic_preds) > 0:
#             # Statistical tests for separation
#             ks_statistic, ks_p_value = stats.ks_2samp(toxic_preds, non_toxic_preds)
#             mannwhitney_stat, mw_p_value = stats.mannwhitneyu(toxic_preds, non_toxic_preds, 
#                                                               alternative='greater')
#         else:
#             ks_statistic, ks_p_value = 0, 1
#             mannwhitney_stat, mw_p_value = 0, 1
        
#         results['distribution_analysis'] = {
#             'toxic_mean': np.mean(toxic_preds) if len(toxic_preds) > 0 else 0,
#             'toxic_std': np.std(toxic_preds) if len(toxic_preds) > 0 else 0,
#             'non_toxic_mean': np.mean(non_toxic_preds) if len(non_toxic_preds) > 0 else 0,
#             'non_toxic_std': np.std(non_toxic_preds) if len(non_toxic_preds) > 0 else 0,
#             'separation_score': np.mean(toxic_preds) - np.mean(non_toxic_preds) if len(toxic_preds) > 0 and len(non_toxic_preds) > 0 else 0,
#             'ks_statistic': ks_statistic,
#             'ks_p_value': ks_p_value,
#             'mannwhitney_stat': mannwhitney_stat,
#             'mw_p_value': mw_p_value
#         }
        
#         # 3. Calibration Analysis
#         try:
#             prob_true, prob_pred = calibration_curve(true_labels, predictions, n_bins=10)
#             calibration_error = np.mean(np.abs(prob_pred - prob_true))
#         except:
#             prob_true, prob_pred = np.array([]), np.array([])
#             calibration_error = float('inf')
            
#         results['calibration'] = {
#             'prob_true': prob_true,
#             'prob_pred': prob_pred,
#             'calibration_error': calibration_error
#         }
        
#         # 4. Confidence Analysis
#         high_conf_mask = (predictions > 0.8) | (predictions < 0.2)
#         if np.sum(high_conf_mask) > 0:
#             high_conf_acc = np.mean((predictions[high_conf_mask] > 0.5) == true_labels[high_conf_mask])
#         else:
#             high_conf_acc = 0.0
            
#         results['confidence_analysis'] = {
#             'high_confidence_ratio': np.mean(high_conf_mask),
#             'high_confidence_accuracy': high_conf_acc,
#         }
        
#         # 5. Error Analysis
#         binary_preds = (predictions > 0.5).astype(int)
#         errors = binary_preds != true_labels
        
#         fp_indices = np.where((binary_preds == 1) & (true_labels == 0))[0]
#         fn_indices = np.where((binary_preds == 0) & (true_labels == 1))[0]
        
#         results['error_analysis'] = {
#             'total_errors': np.sum(errors),
#             'error_rate': np.mean(errors),
#             'false_positives': len(fp_indices),
#             'false_negatives': len(fn_indices),
#             'fp_confidence': np.mean(predictions[fp_indices]) if len(fp_indices) > 0 else 0,
#             'fn_confidence': np.mean(predictions[fn_indices]) if len(fn_indices) > 0 else 0
#         }
        
#         self.evaluation_results[method_name] = results
#         return results
    
#     def create_evaluation_plots(self, predictions, true_labels, method_name, save_dir=None):
#         """Create comprehensive evaluation plots"""
        
#         if save_dir:
#             os.makedirs(save_dir, exist_ok=True)
        
#         fig, axes = plt.subplots(2, 3, figsize=(15, 10))
#         fig.suptitle(f'Steering Vector Evaluation: {method_name}', fontsize=16)
        
#         # 1. ROC Curve
#         fpr, tpr, _ = roc_curve(true_labels, predictions)
#         roc_auc = auc(fpr, tpr)
        
#         axes[0, 0].plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')
#         axes[0, 0].plot([0, 1], [0, 1], 'k--')
#         axes[0, 0].set_xlabel('False Positive Rate')
#         axes[0, 0].set_ylabel('True Positive Rate')
#         axes[0, 0].set_title('ROC Curve')
#         axes[0, 0].legend()
        
#         # 2. Prediction Distribution
#         toxic_preds = predictions[true_labels == 1]
#         non_toxic_preds = predictions[true_labels == 0]
        
#         if len(toxic_preds) > 0 and len(non_toxic_preds) > 0:
#             axes[0, 1].hist(non_toxic_preds, alpha=0.5, label='Non-toxic', bins=30, density=True)
#             axes[0, 1].hist(toxic_preds, alpha=0.5, label='Toxic', bins=30, density=True)
#             axes[0, 1].set_xlabel('Prediction Score')
#             axes[0, 1].set_ylabel('Density')
#             axes[0, 1].set_title('Prediction Distributions')
#             axes[0, 1].legend()
        
#         # 3. Calibration Plot
#         try:
#             prob_true, prob_pred = calibration_curve(true_labels, predictions, n_bins=10)
#             axes[0, 2].plot(prob_pred, prob_true, 's-', label='Model')
#             axes[0, 2].plot([0, 1], [0, 1], 'k--', label='Perfect calibration')
#             axes[0, 2].set_xlabel('Mean Predicted Probability')
#             axes[0, 2].set_ylabel('Fraction of Positives')
#             axes[0, 2].set_title('Calibration Plot')
#             axes[0, 2].legend()
#         except:
#             axes[0, 2].text(0.5, 0.5, 'Calibration plot\nnot available', 
#                            ha='center', va='center', transform=axes[0, 2].transAxes)
        
#         # 4. Confusion Matrix
#         binary_preds = (predictions > 0.5).astype(int)
#         cm = confusion_matrix(true_labels, binary_preds)
#         sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 0])
#         axes[1, 0].set_title('Confusion Matrix')
#         axes[1, 0].set_xlabel('Predicted')
#         axes[1, 0].set_ylabel('Actual')
        
#         # 5. Prediction Scatter
#         colors = ['red' if label == 1 else 'blue' for label in true_labels]
#         axes[1, 1].scatter(range(len(predictions)), predictions, c=colors, alpha=0.5, s=10)
#         axes[1, 1].axhline(y=0.5, color='black', linestyle='--')
#         axes[1, 1].set_xlabel('Sample Index')
#         axes[1, 1].set_ylabel('Prediction Score')
#         axes[1, 1].set_title('Predictions (Red=Toxic, Blue=Non-toxic)')
        
#         # 6. Performance by threshold
#         thresholds = np.linspace(0, 1, 21)
#         accuracies = []
#         f1_scores = []
        
#         for threshold in thresholds:
#             thresh_preds = (predictions > threshold).astype(int)
#             acc = accuracy_score(true_labels, thresh_preds)
#             _, _, f1, _ = precision_recall_fscore_support(true_labels, thresh_preds, average='binary', zero_division=0)
#             accuracies.append(acc)
#             f1_scores.append(f1)
        
#         axes[1, 2].plot(thresholds, accuracies, label='Accuracy', marker='o')
#         axes[1, 2].plot(thresholds, f1_scores, label='F1 Score', marker='s')
#         axes[1, 2].set_xlabel('Threshold')
#         axes[1, 2].set_ylabel('Score')
#         axes[1, 2].set_title('Performance vs Threshold')
#         axes[1, 2].legend()
        
#         plt.tight_layout()
        
#         if save_dir:
#             plt.savefig(f'{save_dir}/evaluation_plots_{method_name}.png', dpi=300, bbox_inches='tight')
        
#         plt.show()
#         return fig
    
#     def print_evaluation_summary(self, results):
#         """Print a summary of evaluation results"""
#         print("\n" + "="*80)
#         print("STEERING VECTOR EVALUATION SUMMARY")
#         print("="*80)
        
#         for method_name, result in results.items():
#             print(f"\nMethod: {method_name}")
#             print("-" * 40)
            
#             # Basic metrics
#             metrics = result['basic_metrics']
#             print("Performance Metrics:")
#             print(f"  AUC:       {metrics['auc']:.4f}")
#             print(f"  F1 Score:  {metrics['f1']:.4f}")
#             print(f"  Precision: {metrics['precision']:.4f}")
#             print(f"  Recall:    {metrics['recall']:.4f}")
#             print(f"  Accuracy:  {metrics['acc']:.4f}")
#             print(f"  MSE:       {metrics['mse']:.4f}")
            
#             # Key effectiveness indicators
#             dist = result['distribution_analysis']
#             print(f"\nSteering Effectiveness:")
#             print(f"  Class Separation Score: {dist['separation_score']:.4f}")
#             print(f"  Statistical Significance (KS p-value): {dist['ks_p_value']:.2e}")
            
#             if result['calibration']['calibration_error'] != float('inf'):
#                 print(f"  Calibration Error: {result['calibration']['calibration_error']:.4f}")
            
#             error = result['error_analysis']
#             print(f"  Error Rate: {error['error_rate']:.4f}")
#             print(f"  False Positive Rate: {error['false_positives']/len(result['basic_metrics']):.4f}")
#             print(f"  False Negative Rate: {error['false_negatives']/len(result['basic_metrics']):.4f}")


# def main():
#     parser = argparse.ArgumentParser(description='Evaluate steering vector effectiveness')
#     parser.add_argument('--vector_path', type=str, required=True,
#                        help='Path to steering vector pickle file')
#     parser.add_argument('--model_name', type=str, default='Meta-Llama-3.1-8B-Instruct')
#     parser.add_argument('--control_method', type=str, default='rfm')
#     parser.add_argument('--prompt_version', type=str, default='empty')
#     parser.add_argument('--n_components', type=int, default=3)
#     parser.add_argument('--save_plots', type=str, default=None,
#                        help='Directory to save evaluation plots')
    
#     args = parser.parse_args()
    
#     # Initialize evaluator
#     evaluator = FlexibleSteeringEvaluator()
    
#     # Run evaluation
#     print("Starting steering vector evaluation...")
#     try:
#         agg_predictions, best_layer_predictions, test_labels = evaluator.evaluate_steering_vector(
#             args.vector_path, args.model_name, args.control_method,
#             args.prompt_version, args.n_components
#         )
        
#         # Evaluate both prediction types
#         results = {}
        
#         print("\nEvaluating aggregated predictions...")
#         agg_results = evaluator.evaluate_predictions(
#             agg_predictions, test_labels, f'{args.control_method}_aggregated'
#         )
#         results[f'{args.control_method}_aggregated'] = agg_results
        
#         if not np.array_equal(agg_predictions, best_layer_predictions):
#             print("Evaluating best layer predictions...")
#             best_results = evaluator.evaluate_predictions(
#                 best_layer_predictions, test_labels, f'{args.control_method}_best_layer'
#             )
#             results[f'{args.control_method}_best_layer'] = best_results
        
#         # Create plots
#         plot_dir = args.save_plots or './evaluation_plots'
#         evaluator.create_evaluation_plots(
#             agg_predictions, test_labels, f'{args.control_method}_aggregated', plot_dir
#         )
        
#         if not np.array_equal(agg_predictions, best_layer_predictions):
#             evaluator.create_evaluation_plots(
#                 best_layer_predictions, test_labels, f'{args.control_method}_best_layer', plot_dir
#             )
        
#         # Print summary
#         evaluator.print_evaluation_summary(results)
        
#         print(f"\nEvaluation completed!")
#         if args.save_plots:
#             print(f"Plots saved to: {plot_dir}")
            
#     except Exception as e:
#         print(f"Error during evaluation: {e}")
#         import traceback
#         traceback.print_exc()


# if __name__ == '__main__':
#     main()


#     # python evaluate_vector.py \
#     # --vector_path /home/ubuntu/llm-research/neural_controllers2/directions/logistic_poetry_llama_3_8b_it.pkl \
#     # --model_name Meta-Llama-3.1-8B-Instruct \
#     # --control_method rfm \
#     # --prompt_version empty \
#     # --n_components 3 \
#     # --save_plots ./evaluation_plots


# python evaluate_vector.py \
#     --model_name "Meta-Llama-3.1-8B-Instruct " \
#     --steering_vectors_path "/home/ubuntu/llm-research/neural_controllers2/directions/logistic_poetry_llama_3_8b_it.pkl" \
#     --layers -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31 \
#     --coef_values 0.45 \
#     --prompt_version empty \
#     --max_samples 500